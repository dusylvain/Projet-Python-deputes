{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import geopandas as gpd\n",
    "import urllib#pour récupérer les données\n",
    "import bs4#pour rendre lisibles les données\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupération des députés sur le site de l'Assemblée Nationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrap (url):\n",
    "    req = urllib.request.Request(url)\n",
    "    html = urllib.request.urlopen(req).read()\n",
    "    page = bs4.BeautifulSoup(html, \"lxml\")\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ToDf (table) :\n",
    "    Listgrossiere=pd.read_html(str(table))#on transforme le tableau en liste de dataframes\n",
    "    listeDf=Listgrossiere[0]#on récupère le dataframe\n",
    "    return listeDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "page=Scrap('http://www2.assemblee-nationale.fr/sycomore/resultats/(offset)/'+str(0)+'/(query)/IiBTRUxFQ1QgbV9jb2RlX2RlcHV0ZSwgaWRfYWN0ZXVyX3RyaWJ1biwgbGVnX21heF90cmlidW4sIG5vbSwgbm9tX2FmZmljaGUsIHByZW5vbSwgZGF0ZV9uYWlzLCBkYXRlX2RlY2VzIEZST00gZGVwdXRlIFdIRVJFIDE9MSAgQU5EIG1fY29kZV9kZXB1dGUgSU4gKFNFTEVDVCBtX2NvZGVfZGVwdXRlIEZST00gbWFuZGF0LCBkZXBhcnRlbWVudCAgV0hFUkUgbWFuZGF0Lm1fbnVtX2RlcCA9IGRlcGFydGVtZW50Lm1fbnVtX2RlcCBBTkQgbWFuZGF0Lm1fdHlwZV9tYW5kYXQgPSAxICkgT1JERVIgQlkgZGF0ZV9uYWlzIERFU0Mi')\n",
    "table = page.find('table')\n",
    "liste=ToDf(table)\n",
    "for i in range(32):\n",
    "    page=Scrap('http://www2.assemblee-nationale.fr/sycomore/resultats/(offset)/'+str((i+1)*500)+'/(query)/IiBTRUxFQ1QgbV9jb2RlX2RlcHV0ZSwgaWRfYWN0ZXVyX3RyaWJ1biwgbGVnX21heF90cmlidW4sIG5vbSwgbm9tX2FmZmljaGUsIHByZW5vbSwgZGF0ZV9uYWlzLCBkYXRlX2RlY2VzIEZST00gZGVwdXRlIFdIRVJFIDE9MSAgQU5EIG1fY29kZV9kZXB1dGUgSU4gKFNFTEVDVCBtX2NvZGVfZGVwdXRlIEZST00gbWFuZGF0LCBkZXBhcnRlbWVudCAgV0hFUkUgbWFuZGF0Lm1fbnVtX2RlcCA9IGRlcGFydGVtZW50Lm1fbnVtX2RlcCBBTkQgbWFuZGF0Lm1fdHlwZV9tYW5kYXQgPSAxICkgT1JERVIgQlkgZGF0ZV9uYWlzIERFU0Mi')\n",
    "    table = page.find('table')#on récupère le seul tableau de la page qui liste des députés\n",
    "    listeDf=ToDf(table)\n",
    "    liste=pd.concat([liste,listeDf])#on récupère le dataframe et on l'ajoute à la liste\n",
    "    \n",
    "liste=liste.reset_index()#on recrée un index afin de ne pas s'arrêter à 500 puis recommencer comme dans les pages web\n",
    "listeDepute=liste[['Nom','Né le']]#on récupère les variables qui nous intéressent  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "listeDepute.to_csv(r'C:\\Users\\sylva\\Desktop\\git\\Projet-Python-deputes\\liste_depute.csv')#on met les données sur csv pour ne pas avoir à rescrapper à chaque fois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse ensuite à la récupération des mandats de chaque député (partie en cours)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_un_mandat(mandat):\n",
    "    #On récupère et lave les titres des colonnes\n",
    "    columns=mandat.findAll('dt')\n",
    "    columns=re.sub('<[^<]+?>', '', str(columns))  \n",
    "    \n",
    "    #On récupère et lave les titres des lignes\n",
    "    lines=mandat.findAll('dd')\n",
    "    lines=re.sub('<[^<]+?>', '', str(lines))\n",
    "    lines=re.sub('\\t', '', str(lines))\n",
    "    lines=re.sub(\"\\n\", '', str(lines))\n",
    "    line=''\n",
    "    \n",
    "    #on s'occupe à un autre endroit du département de mort et de naissance\n",
    "    depute_naiss=page.findAll('dl', {'class' : \"deputes-liste-attributs sycomore-infos-generales\"})\n",
    "    dep_naiss=re.findall('\\((\\w*-?\\w*)\\s', str(depute_naiss))#on récupère les départements\n",
    "    dep_naiss=dep_naiss.reverse()#on connait plus les départements de mort que de naissance, s'il n'y a qu'une donnée c'est donc un département de mort a priori (sinon, l'attachement au territoire est quand même acceptable)\n",
    "    \n",
    "    #On met tout dans un dataframe\n",
    "    columns=columns[1:-1].split(',')\n",
    "    #columns.append('département de naissance')\n",
    "    #columns.append('département de mort')\n",
    "    \"\"\"Lève trop souvent les assertionError ! Il manque souvent au moins l'un des deux, donc il ne prend pas en compte les données\"\"\"\n",
    "    lines=lines[1:-1].split(',')\n",
    "    #lines.append(dep_naiss)#contient un, deux ou aucun élément selon les informations\n",
    "    df=pd.DataFrame([lines],columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecupNom (page):\n",
    "    nom_depute=page.find('title')#on va là où il y a le nom\n",
    "    nom_depute=re.sub('<[^<]+?>', '', str(nom_depute))#on enlève les balises\n",
    "    nom_depute=nom_depute.split()#on récupère les mots du titre\n",
    "    mot=nom_depute[0]#on initialise au premier mot\n",
    "    nom_dep=''#on initialise la variable où on va récupérer le nom\n",
    "    while mot!='-' :#on prend les premiers mots qui composent le nom, avant la suite du titre\n",
    "        nom_dep=nom_dep+' '+mot#on concatène le mot appartenant au nom avec les précédents\n",
    "        nom_depute=nom_depute[1:]#on supprime le mot étudié\n",
    "        mot=nom_depute[0]#pour passer au suivant\n",
    "    return nom_dep#on retourne le mot complet. Il est surement possible de le faire beaucoup plus vite en regex, mais les miennes ne marchent pas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "temps d'exécution 0.5108110904693604\n"
     ]
    }
   ],
   "source": [
    "tps1=time.time()\n",
    "page=Scrap('http://www2.assemblee-nationale.fr/sycomore/fiche/(num_dept)/'+str(5))\n",
    "listeMandat=page\n",
    "liste=[]\n",
    "numProbleme=[]\n",
    "for i in range(5,19501) :\n",
    "    if i%500==1 :#pour mesurer l'avancement du scrapping\n",
    "        print(i)\n",
    "    page=Scrap('http://www2.assemblee-nationale.fr/sycomore/fiche/(num_dept)/'+str(i+7))\n",
    "    #Une petite blague c'est glissé dans le site : les mandats de sénateurs sont présent dans le code html mais ils sont \"cachés\"\n",
    "    #On les enlève donc d'abord\n",
    "    pouf=page.find('div', {'id':\"mandats_an\"})\n",
    "    if pouf :\n",
    "        depute=pouf.findAll('dl', {'class' : \"deputes-liste-attributs sycomore\"})\n",
    "    else :#éviter les cas d'assemblée nationale constituante qui ont un autre nom que mandats_an\n",
    "        depute=page.findAll('dl', {'class' : \"deputes-liste-attributs sycomore\"})\n",
    "    for mandat in depute :\n",
    "        try:\n",
    "            df = scrap_un_mandat(mandat)\n",
    "        except AssertionError :\n",
    "            numProbleme.append(i)\n",
    "        else :#ne marche pas en elif ! A cause du try\n",
    "            df[\"nom du député\"]=RecupNom(page)\n",
    "            if len(liste)==0: #On initialise avec le premier mandat\n",
    "                liste=df  \n",
    "            else:\n",
    "                liste=pd.concat([liste,df])#on fusionne les mandats\n",
    "print(len(numProbleme))\n",
    "tps2=time.time()\n",
    "print(\"temps d'exécution\",tps2-tps1)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y des pages de députés jusqu'à 19500 sur le site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste=liste.rename(columns={' Département': 'Département', ' Groupe': 'Groupe', ' Législature': 'Législature', ' Mandat' :'Mandat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_dep=liste[['Département', 'Groupe', 'Législature', 'Mandat', 'Régime politique', 'nom du député']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_dep.to_csv(r'C:\\Users\\sylva\\Desktop\\git\\Projet-Python-deputes\\liste_mandat.csv')\n",
    "#on met les données sur csv pour ne pas avoir à rescrapper à chaque fois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(liste)/len(numProbleme)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
